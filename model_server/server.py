from fastapi import FastAPI, HTTPException
from pydantic import BaseModel
from pydantic_settings import BaseSettings
from dotenv import load_dotenv

from concurrent.futures import ThreadPoolExecutor
from functools import partial
from pathlib import Path
from threading import Lock
from typing import List, Dict, Optional

import json
import os
import shutil
import sys
import time
import hashlib

import faiss
import numpy as np
from pathspec import PathSpec
from pathspec.patterns import GitWildMatchPattern
from tqdm import tqdm

load_dotenv(Path(__file__).with_name('.env'))

OWL_INDEX_DIR = ".owl_index"

from extractors import extract_functions
from indexer import CodeIndexer
from cluster_index import ClusterIndex

# ãƒ¢ãƒ‡ãƒ«ç®¡ç†ã‚’ model.py ã‹ã‚‰ import
from model import get_model, get_current_device, cleanup_memory, encode_code, DEFAULT_MODEL, get_device

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°
model_name = os.environ.get("OWL_MODEL_NAME", DEFAULT_MODEL)

# äº’æ›æ€§ã®ãŸã‚ã€ä¸€æ™‚çš„ãªãƒ€ãƒŸãƒ¼é–¢æ•°
def get_device_and_prepare():
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ€ãƒŸãƒ¼é–¢æ•° - get_model()ãŒè‡ªå‹•çš„ã«ãƒ‡ãƒã‚¤ã‚¹ç®¡ç†ã‚’è¡Œã†"""
    pass

def encode_with_memory_management(codes: list[str], batch_size: int = None, max_retries: int = 3, show_progress: bool = True):
    """å¾Œæ–¹äº’æ›æ€§ã®ãŸã‚ã®ãƒ©ãƒƒãƒ‘ãƒ¼é–¢æ•°"""
    if batch_size is None:
        batch_size = 8
    return encode_code(codes, batch_size, max_retries, show_progress)

# äº’æ›æ€§ã®ãŸã‚ã€modelå¤‰æ•°ã‚’è¿½åŠ 
model = get_model()
model_device = get_current_device()

app = FastAPI()

# === è¨­å®š: ãƒãƒƒãƒã‚µã‚¤ã‚ºãªã© ===
class OwlSettings(BaseSettings):
    batch_size: int = 8
    
    class Config:
        env_prefix = "OWL_"  # ç’°å¢ƒå¤‰æ•°ã¯OWL_BATCH_SIZEã§è¨­å®šå¯èƒ½

settings = OwlSettings()

# âœ… ãƒªã‚¯ã‚¨ã‚¹ãƒˆç”¨ã® Pydantic ãƒ¢ãƒ‡ãƒ«
class EmbedRequest(BaseModel):
    texts: list[str]

class IndexAndSearchRequest(BaseModel):
    source_code: str
    query_code: str
    top_k: int = 5

class IndexStatus(BaseModel):
    directory: str
    indexed_files: List[str]
    last_indexed: float
    up_to_date: bool

class BuildIndexRequest(BaseModel):
    directory: str
    file_ext: str = ".py"

class SearchFunctionsSimpleRequest(BaseModel):
    directory: str
    query: str
    top_k: int = 5
    file_ext: str = ".py"

class FunctionRangeRequest(BaseModel):
    file: str
    func_name: str

# ã‚¯ãƒ©ã‚¹çµ±è¨ˆè¡¨ç¤ºç”¨ã®ãƒªã‚¯ã‚¨ã‚¹ãƒˆãƒ¢ãƒ‡ãƒ«
class ClassStatsRequest(BaseModel):
    directory: str
    query: str  # æ¤œç´¢ã‚¯ã‚¨ãƒª
    top_k: int = 50  # ä¸Šä½ä½•ä»¶ã®é–¢æ•°ã‚’å–å¾—ã™ã‚‹ã‹
    file_ext: str = ".py"

# ã‚µãƒ¼ãƒãƒ¼å…¨ä½“ã§1ã¤ã®ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’ä¿æŒ
index_lock = Lock()
global_indexer = None

# ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æƒ…å ±ã‚’ä¿æŒã™ã‚‹ã‚¯ãƒ©ã‚¹
class GlobalIndexerState:
    def __init__(self):
        self.indexer: Optional[CodeIndexer] = None
        self.file_info: Dict[str, Dict[str, float | str]] = {}  # mtimeã¨hashã‚’ä¿æŒ
        self.directory: Optional[str] = None
        self.last_indexed: float = 0.0
        self.file_ext: str = ".py"
        self.embeddings: Optional[np.ndarray] = None  # è¿½åŠ : é–¢æ•°åŸ‹ã‚è¾¼ã¿
        self.faiss_index: Optional[faiss.IndexFlatL2] = None  # è¿½åŠ : FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹
        self.index_dir = None  # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã”ã¨ã«å‹•çš„ã«è¨­å®š
        self.model_name: Optional[str] = None  # è¿½åŠ : ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹æ§‹ç¯‰ã«ä½¿ç”¨ã—ãŸãƒ¢ãƒ‡ãƒ«å
        self.model_config: dict = {}  # è¿½åŠ : ãƒ¢ãƒ‡ãƒ«æ§‹æˆæƒ…å ±

    def get_current_model_config(self) -> dict:
        # Add new config keys here as needed for extensibility
        return {
            "model_name": model_name,
            # e.g. add more: "embedding_dim": ..., "other_param": ...
        }

    def set_index_dir(self, directory: str, file_ext: str = ".py"):
        safe_dir = os.path.basename(os.path.abspath(directory))
        ext_dir = file_ext.lstrip(".")
        self.index_dir = os.path.join(os.getcwd(), OWL_INDEX_DIR, safe_dir, ext_dir)
        # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã®ä½œæˆã¯ä¿å­˜æ™‚ã®ã¿è¡Œã†ï¼ˆstartupæ™‚ã¯ä½œæˆã—ãªã„ï¼‰

    def is_up_to_date(self, tol: float = 1e-3, directory: Optional[str] = None) -> bool:
        # directoryå¼•æ•°ãŒæŒ‡å®šã•ã‚ŒãŸå ´åˆã¯ãã‚Œã¨self.directoryã‚’æ¯”è¼ƒ
        if directory is not None:
            if os.path.abspath(directory) != os.path.abspath(self.directory or ""):
                print(f"[is_up_to_date] directory mismatch: {directory} != {self.directory}")
                return False
        # Compare full model_config for extensibility
        current_model_config = self.get_current_model_config()
        if self.model_config and self.model_config != current_model_config:
            print(f"[is_up_to_date] model_config mismatch: {self.model_config} != {current_model_config}")
            return False
        if not self.directory:
            print("[is_up_to_date] self.directory is None")
            return False
        for path, info in self.file_info.items():
            if not os.path.exists(path):
                print(f"[is_up_to_date] file missing: {path}")
                return False
            mtime = os.path.getmtime(path)
            hash_now = file_hash(path)
            if abs(mtime - info["mtime"]) > tol or hash_now != info["hash"]:
                print(f"[is_up_to_date] file changed: {path}")
                return False
        return True

    def clear_cache(self, clear_disk: bool = False):
        """ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å¼·åˆ¶çš„ã«å†æ§‹ç¯‰ã‚’ä¿ƒã™"""
        self.indexer = None
        self.embeddings = None
        self.faiss_index = None
        self.file_info = {}
        self.directory = None
        self.last_indexed = 0.0
        self.model_name = None
        self.model_config = {}
        if clear_disk and self.index_dir and os.path.exists(self.index_dir):
            shutil.rmtree(self.index_dir, ignore_errors=True)

    def force_rebuild_from_disk(self, directory: str, file_ext: str = ".py"):
        """ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰å¼·åˆ¶çš„ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰"""
        self.clear_cache()
        self.load(directory, file_ext)

    def save(self):
        if not self.directory:
            return
        self.set_index_dir(self.directory, self.file_ext)
        if not self.index_dir:
            return
        os.makedirs(self.index_dir, exist_ok=True)
        # é–¢æ•°ãƒªã‚¹ãƒˆ
        if self.indexer:
            with open(os.path.join(self.index_dir, "functions.json"), "w", encoding="utf-8") as f:
                json.dump(self.indexer.functions, f, ensure_ascii=False)
        # åŸ‹ã‚è¾¼ã¿
        if self.embeddings is not None:
            np.save(os.path.join(self.index_dir, "embeddings.npy"), self.embeddings)
        # faiss
        if self.faiss_index is not None:
            faiss.write_index(self.faiss_index, os.path.join(self.index_dir, "faiss.index"))
        # ãã®ä»–ãƒ¡ã‚¿
        meta = {
            "file_info": self.file_info,
            "directory": os.path.abspath(self.directory) if self.directory else None,
            "last_indexed": self.last_indexed,
            "file_ext": self.file_ext,
            "model_name": self.model_name or model_name,
            "model_config": self.model_config or self.get_current_model_config(),
        }
        with open(os.path.join(self.index_dir, "meta.json"), "w", encoding="utf-8") as f:
            json.dump(meta, f, ensure_ascii=False)

    def load(self, directory: str, file_ext: str = ".py"):
        directory = os.path.abspath(directory)
        self.set_index_dir(directory, file_ext)
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªãŒå­˜åœ¨ã—ãªã„å ´åˆã¯ä½•ã‚‚ã—ãªã„ï¼ˆãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆï¼‰
        if not os.path.exists(self.index_dir):
            self.indexer = None
            self.embeddings = None
            self.faiss_index = None
            self.file_info = {}
            self.directory = None
            self.last_indexed = 0.0
            self.file_ext = ".py"
            self.model_name = None
            self.model_config = {}
            return
        try:
            with open(os.path.join(self.index_dir, "functions.json"), "r", encoding="utf-8") as f:
                functions = json.load(f)
            self.indexer = CodeIndexer()
            self.indexer.add_functions(functions)
        except Exception:
            self.indexer = None
        try:
            self.embeddings = np.load(os.path.join(self.index_dir, "embeddings.npy"))
        except Exception:
            self.embeddings = None
        try:
            self.faiss_index = faiss.read_index(os.path.join(self.index_dir, "faiss.index"))
        except Exception:
            self.faiss_index = None
        try:
            with open(os.path.join(self.index_dir, "meta.json"), "r", encoding="utf-8") as f:
                meta = json.load(f)
            self.file_info = meta.get("file_info", {})
            self.directory = os.path.abspath(meta.get("directory", directory))
            self.last_indexed = meta.get("last_indexed", 0.0)
            self.file_ext = meta.get("file_ext", ".py")
            self.model_name = meta.get("model_name")
            self.model_config = meta.get("model_config", {"model_name": self.model_name})
        except Exception:
            self.file_info = {}
            self.directory = None
            self.last_indexed = 0.0
            self.file_ext = ".py"
            self.model_name = None
            self.model_config = {}

global_index_state = GlobalIndexerState()
# ã‚µãƒ¼ãƒãƒ¼èµ·å‹•æ™‚ã¯è‡ªå‹•ãƒ­ãƒ¼ãƒ‰ã‚’è¡Œã‚ãªã„ï¼ˆãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥å„ªå…ˆã€å¿…è¦æ™‚ã®ã¿ãƒ‡ã‚£ã‚¹ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ï¼‰

def load_gitignore_spec(root_dir: str) -> Optional[PathSpec]:
    """
    æŒ‡å®šãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªç›´ä¸‹ã® .gitignore ã‚’èª­ã¿è¾¼ã¿ã€Git ã®ãƒ¯ã‚¤ãƒ«ãƒ‰ã‚«ãƒ¼ãƒ‰ä»•æ§˜
    ã‚’ãã®ã¾ã¾è§£é‡ˆã§ãã‚‹ PathSpec ã‚’è¿”ã™ã€‚å­˜åœ¨ã—ãªã‘ã‚Œã° Noneã€‚
    """
    gi_path = os.path.join(root_dir, ".gitignore")
    if not os.path.exists(gi_path):
        return None
    with open(gi_path, encoding="utf-8") as f:
        lines = [ln.rstrip("\n") for ln in f if ln.strip() and not ln.startswith("#")]
    return PathSpec.from_lines(GitWildMatchPattern, lines)

def is_ignored(path: str, spec: Optional[PathSpec], root_dir: str) -> bool:
    if spec is None:
        return False
    rel_path = os.path.relpath(path, root_dir)
    return spec.match_file(rel_path)

# ãƒ•ã‚¡ã‚¤ãƒ«ã®ãƒãƒƒã‚·ãƒ¥è¨ˆç®—é–¢æ•°
def file_hash(path):
    h = hashlib.sha256()
    with open(path, "rb") as f:
        while True:
            chunk = f.read(8192)
            if not chunk:
                break
            h.update(chunk)
    return h.hexdigest()

# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå†…ã®å…¨ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é–¢æ•°æŠ½å‡ºãƒ»ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆï¼ˆä¸€æ™‚çš„ãªã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã€çŠ¶æ…‹ä¿å­˜ãªã—ï¼‰
def build_index(directory: str, file_ext: str = ".py", max_workers: int = 8, update_state: bool = False):
    def func_id(func):
        # ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹ãƒ»é–¢æ•°åãƒ»linenoãƒ»end_linenoã‚’çµ„ã¿åˆã‚ã›ã¦ä¸€æ„ãªIDã‚’ç”Ÿæˆ
        key = f"{func.get('file','')}|{func.get('name','')}|{func.get('lineno','')}|{func.get('end_lineno','')}"
        return hashlib.sha256(key.encode()).hexdigest()

    directory = os.path.abspath(directory)
    current_model_config = global_index_state.get_current_model_config()
    # Always check model_config for cache validity
    if (
        global_index_state.indexer is not None and 
        global_index_state.directory == directory and
        global_index_state.file_ext == file_ext and
        global_index_state.is_up_to_date(directory=directory)
    ):
        # If model_config changed, force clear and rebuild
        if global_index_state.model_config and global_index_state.model_config != current_model_config:
            print("[build_index] model_config mismatch â€“ rebuilding")
            global_index_state.clear_cache(clear_disk=True)
        else:
            print(f"[build_index] ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ€æ–°ã€ãƒ‡ã‚£ã‚¹ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ— (funcs={len(global_index_state.indexer.functions)}, files={len(global_index_state.file_info)})")
            return (global_index_state.indexer.functions, 
                    len(global_index_state.file_info), 
                    global_index_state.indexer)
    # If we get here, we need to rebuild
    # Always update model_config for extensibility
    global_index_state.model_config = current_model_config
    # å¿…è¦ãªæ™‚ã®ã¿ãƒ‡ã‚£ã‚¹ã‚¯ã‹ã‚‰ãƒ­ãƒ¼ãƒ‰
    global_index_state.load(directory, file_ext)
    global_index_state.set_index_dir(directory, file_ext)
    if global_index_state.model_config and global_index_state.model_config != current_model_config:
        print("[build_index] model_config mismatch after load â€“ rebuilding")
        global_index_state.clear_cache(clear_disk=True)
        # After clearing, reload config
        global_index_state.model_config = current_model_config
    if global_index_state.model_name and global_index_state.model_name != model_name:
        print("[build_index] cached model mismatch â€“ rebuilding")
        global_index_state.clear_cache(clear_disk=True)
        prev_info = {}
        prev_indexer = None
        prev_funcs_by_file = {}
    else:
        prev_info = dict(global_index_state.file_info) if update_state else {}
        prev_indexer = global_index_state.indexer if update_state else None
        prev_funcs_by_file = {}
        if prev_indexer:
            for func in prev_indexer.functions:
                prev_funcs_by_file.setdefault(func.get("file"), []).append(func)
    spec = load_gitignore_spec(directory)
    file_paths = []
    for root, dirs, files in os.walk(directory):
        dirs[:] = [d for d in dirs if not is_ignored(os.path.join(root, d), spec, directory)]
        for fname in files:
            if not fname.endswith(file_ext):
                continue
            fpath = os.path.join(root, fname)
            if is_ignored(fpath, spec, directory):
                continue
            file_paths.append(fpath)

    # è¿½åŠ ãƒ»å¤‰æ›´ãƒ»å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«ã‚’åˆ¤å®šï¼ˆmtimeã¨hashä¸¡æ–¹ã§åˆ¤å®šï¼‰
    new_info = {f: {"mtime": os.path.getmtime(f), "hash": file_hash(f)} for f in file_paths}
    added_or_modified = [f for f in file_paths if f not in prev_info or prev_info[f]["hash"] != new_info[f]["hash"]]
    unchanged = [f for f in file_paths if f in prev_info and prev_info[f]["hash"] == new_info[f]["hash"]]
    deleted = [f for f in prev_info if f not in new_info]

    # å¤‰æ›´ãƒ»å‰Šé™¤ã‚¼ãƒ­ãªã‚‰ä½•ã‚‚ã—ãªã„ã§æˆ»ã‚‹ï¼ˆã‚­ãƒ£ãƒƒã‚·ãƒ¥å†åˆ©ç”¨ï¼‰
    if update_state and not added_or_modified and not deleted:
        print("[build_index] up-to-date â‡’ cache reuse")
        return (
            global_index_state.indexer.functions,
            len(global_index_state.file_info),
            global_index_state.indexer,
        )

    # è¿½åŠ ãƒ»å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å†æŠ½å‡º
    def process_file(fpath):
        try:
            funcs = extract_functions(fpath)
            for func in funcs:
                func["file"] = fpath
            return funcs
        except Exception as e:
            print(f"âš ï¸ {fpath}: {e}")
            return []

    results = []
    for f in unchanged:
        results.extend(prev_funcs_by_file.get(f, []))
    added_modified_funcs = []
    if added_or_modified:
        if len(added_or_modified) < 16:
            for fpath in tqdm(added_or_modified, desc="Indexing (serial, diff)", disable=False, file=sys.stdout):
                added_modified_funcs.extend(process_file(fpath))
        else:
            with ThreadPoolExecutor(max_workers=max_workers) as executor:
                for res in tqdm(executor.map(process_file, added_or_modified), total=len(added_or_modified), desc="Indexing (parallel, diff)", disable=False, file=sys.stdout):
                    added_modified_funcs.extend(res)
        results.extend(added_modified_funcs)

    # --- å·®åˆ†é«˜é€ŸåŒ–: å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«ãŒã‚ã£ã¦ã‚‚å…¨å†ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã‚’é¿ã‘ã‚‹ ---
    if update_state:
        # 1. æ—¢å­˜é–¢æ•°IDãƒªã‚¹ãƒˆ
        prev_funcs = prev_indexer.functions if prev_indexer else []
        prev_func_ids = [func_id(f) for f in prev_funcs]
        prev_func_id2idx = {fid: i for i, fid in enumerate(prev_func_ids)}
        # 2. å‰Šé™¤ãƒ•ã‚¡ã‚¤ãƒ«ã«å«ã¾ã‚Œã‚‹é–¢æ•°ID
        deleted_func_ids = set()
        for f in deleted:
            for func in prev_funcs_by_file.get(f, []):
                deleted_func_ids.add(func_id(func))
        # 3. å¤‰æ›´ã®ãªã„é–¢æ•°ID
        unchanged_func_ids = [func_id(f) for f in results]
        # 4. è¿½åŠ ãƒ»å¤‰æ›´åˆ†ã®é–¢æ•°ID
        added_func_ids = [func_id(f) for f in added_modified_funcs]
        # 5. æ–°ã—ã„å…¨é–¢æ•°ãƒªã‚¹ãƒˆ
        new_funcs = results
        # 6. æ–°ã—ã„å…¨é–¢æ•°IDãƒªã‚¹ãƒˆ
        new_func_ids = unchanged_func_ids + added_func_ids
        # 7. åŸ‹ã‚è¾¼ã¿ã®å†æ§‹ç¯‰
        if prev_indexer and global_index_state.embeddings is not None and global_index_state.faiss_index is not None:
            # æ—¢å­˜åŸ‹ã‚è¾¼ã¿ã‹ã‚‰å‰Šé™¤åˆ†ã‚’é™¤å¤–
            prev_embeddings = global_index_state.embeddings
            keep_indices = [prev_func_id2idx[fid] for fid in unchanged_func_ids if fid in prev_func_id2idx]
            kept_embeddings = prev_embeddings[keep_indices] if keep_indices else np.zeros((0, prev_embeddings.shape[1]), dtype=prev_embeddings.dtype)
            # è¿½åŠ ãƒ»å¤‰æ›´åˆ†ã®åŸ‹ã‚è¾¼ã¿
            if added_modified_funcs:
                new_codes = [func["code"] for func in added_modified_funcs]
                print(f"ğŸ”„ Generating embeddings for {len(new_codes)} new/modified functions...")
                new_embeddings = encode_code(new_codes, settings.batch_size, show_progress=True)
                embeddings = np.vstack([kept_embeddings, new_embeddings]) if kept_embeddings.shape[0] > 0 else new_embeddings
            else:
                embeddings = kept_embeddings
            # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰
            if embeddings is not None and embeddings.shape[0] > 0:
                faiss_index = faiss.IndexFlatL2(embeddings.shape[1])
                faiss_index.add(embeddings)
            else:
                faiss_index = None
                embeddings = None
            global_index_state.embeddings = embeddings
            global_index_state.faiss_index = faiss_index
        else:
            # åˆå› or ã‚­ãƒ£ãƒƒã‚·ãƒ¥ãªã—: å…¨ä»¶ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰
            codes = [func["code"] for func in new_funcs]
            if codes:
                print(f"ğŸ”„ Generating embeddings for {len(codes)} functions...")
                embeddings = encode_code(codes, settings.batch_size, show_progress=True)
                faiss_index = faiss.IndexFlatL2(embeddings.shape[1])
                faiss_index.add(embeddings)
                global_index_state.embeddings = embeddings
                global_index_state.faiss_index = faiss_index
            else:
                global_index_state.embeddings = None
                global_index_state.faiss_index = None
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ãƒ»ãƒ¡ã‚¿æƒ…å ±æ›´æ–°
        indexer = CodeIndexer()
        indexer.add_functions(new_funcs)
        global_index_state.indexer = indexer
        global_index_state.directory = os.path.abspath(directory)
        global_index_state.file_ext = file_ext
        global_index_state.last_indexed = float(time.time())
        global_index_state.file_info = new_info
        global_index_state.model_name = model_name
        global_index_state.save()
    else:
        indexer = CodeIndexer()
        indexer.add_functions(results)
    return results, len(file_paths), indexer

@app.post("/embed")
async def embed(req: EmbedRequest):
    print("/embed called")
    embeddings = encode_with_memory_management(req.texts, settings.batch_size)
    return {"embeddings": embeddings.tolist()}

@app.post("/index_and_search")
async def index_and_search(req: IndexAndSearchRequest):
    print("/index_and_search called")
    global global_indexer
    with index_lock:
        if global_indexer is None:
            # åˆå›ã®ã¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ä½œæˆ
            functions = extract_functions(req.source_code)
            if not functions:
                return {"results": []}
            global_indexer = CodeIndexer()
            global_indexer.add_functions(functions)
        # æ—¢å­˜ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã§æ¤œç´¢
        results = global_indexer.search(req.query_code, top_k=req.top_k)
    return {"results": results}

@app.post("/build_index")
async def build_index_api(req: BuildIndexRequest):
    print(f"/build_index called for directory: {req.directory}")
    with index_lock:
        results, file_count, _ = build_index(req.directory, req.file_ext, update_state=True)
    return {"num_functions": len(results), "num_files": file_count}

@app.post("/force_rebuild_index")
async def force_rebuild_index_api(req: BuildIndexRequest):
    """ã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ã‚¯ãƒªã‚¢ã—ã¦å¼·åˆ¶çš„ã«ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å†æ§‹ç¯‰"""
    print(f"/force_rebuild_index called for directory: {req.directory}")
    with index_lock:
        global_index_state.clear_cache()
        results, file_count, _ = build_index(req.directory, req.file_ext, update_state=True)
    return {"num_functions": len(results), "num_files": file_count, "message": "Index forcefully rebuilt"}

@app.get("/index_status")
async def index_status():
    print("/index_status called")
    # If no directory is set, consider it up to date (no index to check)
    if global_index_state.directory is None:
        up_to_date = True
    else:
        up_to_date = global_index_state.is_up_to_date()
    return IndexStatus(
        directory=global_index_state.directory or "",
        indexed_files=list(global_index_state.file_info.keys()),
        last_indexed=global_index_state.last_indexed,
        up_to_date=up_to_date
    )

@app.post("/search")
async def search_api(query: str, top_k: int = 5):
    print(f"/search called with query: {query}")
    with index_lock:
        if not global_index_state.indexer:
            return {"results": [], "error": "No index built."}
        results = global_index_state.indexer.search(query, top_k=top_k)
    return {"results": results}

# --- ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å‰²ãƒ»ClusterManageråˆ©ç”¨ã®é››å½¢ ---
# ã‚°ãƒ­ãƒ¼ãƒãƒ«ã§ã‚¯ãƒ©ã‚¹ã‚¿ãƒãƒãƒ¼ã‚¸ãƒ£ã‚’ä¿æŒï¼ˆæœ¬å®Ÿè£…ã§ã¯ãƒªã‚¯ã‚¨ã‚¹ãƒˆã”ã¨ã«å†æ§‹ç¯‰ã›ãšã‚­ãƒ£ãƒƒã‚·ãƒ¥æ¨å¥¨ï¼‰

global_cluster_manager = None

def get_or_create_cluster_manager(directory: str, file_ext: str = ".py"):
    global global_cluster_manager
    if (
        global_cluster_manager is None
        or global_cluster_manager.base_dir != directory
        or global_cluster_manager.file_ext != file_ext
    ):
        global_cluster_manager = ClusterManager(directory, file_ext)
    return global_cluster_manager

@app.post("/search_functions")
async def search_functions_api(directory: str, query: str, top_k: int = 5, file_ext: str = ".py"):
    """ã‚¯ãƒ©ã‚¹ã‚¿ãƒ™ãƒ¼ã‚¹ã®æ¤œç´¢API"""
    with index_lock:
        cluster_manager = get_or_create_cluster_manager(directory, file_ext)
        # å·®åˆ†æ›´æ–°ã‚’å®Ÿè¡Œ
        cluster_manager.update_all_clusters()
        # æ¤œç´¢å®Ÿè¡Œ
        results = cluster_manager.search_in_clusters(query, top_k)
        return {"results": results, "num_clusters": len(cluster_manager.cluster_indexes)}

@app.post("/build_cluster_index")
async def build_cluster_index_api(req: BuildIndexRequest):
    """ã‚¯ãƒ©ã‚¹ã‚¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ§‹ç¯‰"""
    print(f"/build_cluster_index called for directory: {req.directory}")
    with index_lock:
        cluster_manager = get_or_create_cluster_manager(req.directory, req.file_ext)
        cluster_manager.update_all_clusters(force_rebuild=True)
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
        total_functions = 0
        total_files = 0
        for cluster in cluster_manager.cluster_indexes.values():
            total_functions += len(cluster.meta)
            total_files += len(cluster.file_map)
        
        return {
            "num_functions": total_functions,
            "num_files": total_files,
            "num_clusters": len(cluster_manager.cluster_indexes),
            "clusters": list(cluster_manager.cluster_indexes.keys())
        }

@app.post("/update_cluster_index")
async def update_cluster_index_api(req: BuildIndexRequest):
    """ã‚¯ãƒ©ã‚¹ã‚¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’å·®åˆ†æ›´æ–°"""
    print(f"/update_cluster_index called for directory: {req.directory}")
    with index_lock:
        cluster_manager = get_or_create_cluster_manager(req.directory, req.file_ext)
        cluster_manager.update_all_clusters(force_rebuild=False)
        
        # çµ±è¨ˆæƒ…å ±ã‚’è¨ˆç®—
        total_functions = 0
        total_files = 0
        updated_clusters = []
        for cluster_name, cluster in cluster_manager.cluster_indexes.items():
            total_functions += len(cluster.meta)
            total_files += len(cluster.file_map)
            # å¤‰æ›´ãŒã‚ã£ãŸã‚¯ãƒ©ã‚¹ã‚¿ã‚’è¨˜éŒ²
            files = cluster_manager.clusters.get(cluster_name, [])
            if not cluster.is_up_to_date(files):
                updated_clusters.append(cluster_name)
        
        return {
            "num_functions": total_functions,
            "num_files": total_files,
            "num_clusters": len(cluster_manager.cluster_indexes),
            "updated_clusters": updated_clusters
        }

def build_index_and_search(directory: str, query: str, file_ext: str = ".py", top_k: int = 5, max_workers: int = 8):
    # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã§æœ€æ–°ãªã‚‰ã€ãƒ‡ã‚£ã‚¹ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—
    if (
        global_index_state.indexer is not None and
        global_index_state.directory == directory and
        global_index_state.file_ext == file_ext and
        global_index_state.is_up_to_date(directory=directory) and
        global_index_state.embeddings is not None and
        global_index_state.faiss_index is not None
    ):
        results = global_index_state.indexer.functions
        embeddings = global_index_state.embeddings
        faiss_index = global_index_state.faiss_index
        file_count = len(global_index_state.file_info)
    else:
        results, file_count, indexer = build_index(directory, file_ext, max_workers, update_state=True)
        embeddings = global_index_state.embeddings
        faiss_index = global_index_state.faiss_index
    if not results or embeddings is None or faiss_index is None:
        return {"results": [], "message": "No functions found."}
    get_device_and_prepare()
    query_emb = model.encode([query], batch_size=settings.batch_size, convert_to_numpy=True)
    D, I = faiss_index.search(query_emb, top_k)
    found = []
    for idx in I[0]:
        if 0 <= idx < len(results):
            found.append(results[idx])
    return {"results": found, "num_functions": len(results), "num_files": file_count}

@app.post("/search_functions_simple")
async def search_functions_simple_api(req: SearchFunctionsSimpleRequest):
    print("indexer_exists:", global_index_state.indexer is not None)
    print("up_to_date:", global_index_state.is_up_to_date())
    print("embeddings_cached:", global_index_state.embeddings is not None)
    print("file_ext:", global_index_state.file_ext)
    with index_lock:
        # ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ãŒæœ‰åŠ¹ã§æœ€æ–°ãªã‚‰ã€ãƒ‡ã‚£ã‚¹ã‚¯ã‚¢ã‚¯ã‚»ã‚¹ã‚’ã‚¹ã‚­ãƒƒãƒ—
        if (
            global_index_state.indexer is not None and
            global_index_state.directory == req.directory and
            global_index_state.file_ext == req.file_ext and
            global_index_state.is_up_to_date(directory=req.directory) and
            global_index_state.embeddings is not None and
            global_index_state.faiss_index is not None
        ):
            print("[search_functions_simple] ãƒ¡ãƒ¢ãƒªã‚­ãƒ£ãƒƒã‚·ãƒ¥ã‚’ä½¿ç”¨")
            results = global_index_state.indexer.functions
            embeddings = global_index_state.embeddings
            faiss_index = global_index_state.faiss_index
            file_count = len(global_index_state.file_info)
        else:
            print("[search_functions_simple] ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰")
            results, file_count, indexer = build_index(req.directory, req.file_ext, update_state=True)
            embeddings = global_index_state.embeddings
            faiss_index = global_index_state.faiss_index
        
        if not results or embeddings is None or faiss_index is None:
            return {"results": [], "message": "No functions found."}
        
        query_emb = encode_code([req.query], batch_size=1)  # ã‚¯ã‚¨ãƒªã¯1ã¤ãªã®ã§ãƒãƒƒãƒã‚µã‚¤ã‚º1
        D, I = faiss_index.search(query_emb, req.top_k)
        found = []
        for idx in I[0]:
            if 0 <= idx < len(results):
                found.append(results[idx])
        return {"results": found, "num_functions": len(results), "num_files": file_count}

@app.post("/get_function_range")
async def get_function_range(req: FunctionRangeRequest):
    """
    æŒ‡å®šãƒ•ã‚¡ã‚¤ãƒ«ãƒ»é–¢æ•°åã«è©²å½“ã™ã‚‹é–¢æ•°ã®é–‹å§‹ãƒ»çµ‚äº†è¡Œç•ªå·ã‚’è¿”ã™API
    """
    try:
        funcs = extract_functions(req.file)
    except Exception as e:
        raise HTTPException(status_code=500, detail=f"extract_functions error: {e}")
    for func in funcs:
        if func.get("name") == req.func_name:
            return {
                "start_line": func.get("lineno"),
                "end_line": func.get("end_lineno")
            }
    raise HTTPException(status_code=404, detail="Function not found")

@app.post("/get_class_stats")
async def get_class_stats(request: ClassStatsRequest):
    try:
        # ã¾ãšæ¤œç´¢ã‚’å®Ÿè¡Œã—ã¦æ¤œç´¢çµæœã‚’å–å¾—
        search_request = SearchFunctionsSimpleRequest(
            directory=request.directory,
            query=request.query,
            top_k=request.top_k,
            file_ext=request.file_ext
        )
        search_response = await search_functions_simple_api(search_request)
        search_results = search_response["results"]
        
        # å…¨ã¦ã®é–¢æ•°ã‚’æŠ½å‡º
        all_functions = []
        directory = request.directory
        ignore_spec = load_gitignore_spec(directory)
        
        files = []
        for root, dirs, filenames in os.walk(directory):
            # .gitignoreã®ãƒ‘ã‚¿ãƒ¼ãƒ³ã§ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã‚’ãƒ•ã‚£ãƒ«ã‚¿
            dirs[:] = [d for d in dirs if not is_ignored(os.path.join(root, d), ignore_spec, directory)]
            
            for filename in filenames:
                if filename.endswith(request.file_ext):
                    file_path = os.path.join(root, filename)
                    if not is_ignored(file_path, ignore_spec, directory):
                        files.append(file_path)
        
        for file_path in files:
            try:
                functions = extract_functions(file_path)
                for func in functions:
                    func['file_path'] = file_path
                    all_functions.append(func)
            except Exception as e:
                print(f"Error processing {file_path}: {e}")
                continue
        
        # ã‚¯ãƒ©ã‚¹åˆ¥ã«ã‚°ãƒ«ãƒ¼ãƒ—åŒ–ï¼ˆãƒ•ã‚¡ã‚¤ãƒ«ã”ã¨ã«ç®¡ç†ï¼‰
        classes = {}
        standalone_functions = []
        
        for func in all_functions:
            if func.get("class_name"):
                class_name = func["class_name"]
                file_path = func.get("file_path", func.get("file", ""))
                class_key = (class_name, file_path)
                if class_key not in classes:
                    classes[class_key] = {
                        "name": class_name,
                        "file_path": file_path,
                        "methods": [],
                        "method_count": 0
                    }
                classes[class_key]["methods"].append(func)
                classes[class_key]["method_count"] += 1
            else:
                standalone_functions.append(func)
        
        print(f"Found {len(classes)} classes and {len(standalone_functions)} standalone functions")
        for class_key, class_info in classes.items():
            print(f"Class {class_info['name']} in {class_info['file_path']}: {class_info['method_count']} methods")
        
        # æ¤œç´¢çµæœãƒ™ãƒ¼ã‚¹ã®é‡ã¿ä»˜ã‘ã‚¹ã‚³ã‚¢è¨ˆç®—
        for class_key, class_info in classes.items():
            search_result_ranks = []
            matched_methods = set()

            for method in class_info["methods"]:
                method_file = method.get("file_path", method.get("file", ""))
                method_file_abs = os.path.abspath(method_file) if method_file else ""
                method_lineno = method.get("lineno", method.get("line_number", 0))
                method_key = (method["name"], method_file_abs, method_lineno)

                method_rank = None
                for i, result in enumerate(search_results):
                    result_func_name = result["name"]
                    result_file = result.get("file", "")
                    result_lineno = result.get("lineno", result.get("line_number", 0))
                    result_file_abs = os.path.abspath(result_file) if result_file else ""

                    if (method["name"] == result_func_name and
                        method_file_abs == result_file_abs and
                        method_lineno == result_lineno):
                        method_rank = i + 1
                        if method_key not in matched_methods:
                            search_result_ranks.append(method_rank)
                            matched_methods.add(method_key)
                            print(f"Matched method {method['name']} in class {class_info['name']} at rank {method_rank}")
                        break

                method["search_rank"] = method_rank

            if search_result_ranks:
                sum_inverse_ranks = sum(1.0 / rank for rank in search_result_ranks)
                weighted_score = sum_inverse_ranks
                best_rank = min(search_result_ranks)  # æœ€é«˜é †ä½ï¼ˆæœ€å°ã®ãƒ©ãƒ³ã‚¯å€¤ï¼‰
            else:
                weighted_score = 0.0
                best_rank = None
            
            proportion = len(search_result_ranks) / class_info["method_count"] if class_info["method_count"] > 0 else 0
            
            # è¤‡åˆã‚¹ã‚³ã‚¢: weighted_score * (1 + proportion_bonus)
            # proportion_bonusã¯å‰²åˆã«åŸºã¥ããƒœãƒ¼ãƒŠã‚¹ï¼ˆ0.0ï½1.0ã®ç¯„å›²ã§æœ€å¤§100%ã®ãƒœãƒ¼ãƒŠã‚¹ï¼‰
            proportion_bonus = proportion * 1.0  # 100%ãƒ’ãƒƒãƒˆãªã‚‰100%ãƒœãƒ¼ãƒŠã‚¹
            composite_score = (weighted_score * (1 + proportion_bonus))/2
            
            class_info["weighted_score"] = weighted_score
            class_info["search_hits"] = len(search_result_ranks)
            class_info["all_ranks"] = search_result_ranks
            class_info["best_rank"] = best_rank
            class_info["proportion"] = proportion
            class_info["composite_score"] = composite_score

            # ã‚¯ãƒ©ã‚¹å†…ãƒ¡ã‚½ãƒƒãƒ‰ã‚’æ¤œç´¢é †ä½ã§ã‚½ãƒ¼ãƒˆï¼ˆé †ä½ãŒãªã„ã‚‚ã®ã¯æœ«å°¾ï¼‰
            class_info["methods"] = sorted(
                class_info["methods"],
                key=lambda m: m.get("search_rank") if m.get("search_rank") is not None else float("inf")
            )
        
        sorted_classes = sorted(classes.values(), key=lambda x: x["composite_score"], reverse=True)
        
        # --- ã“ã“ã‹ã‚‰ standalone_functions ã‚’ãƒ©ãƒ³ã‚­ãƒ³ã‚°é †ã«ã‚½ãƒ¼ãƒˆ ---
        # æ¤œç´¢çµæœã® (name, file_path, lineno) ã§ä¸€è‡´ã™ã‚‹ã‚‚ã®ã‚’å…ˆé ­ã«ä¸¦ã¹ã‚‹
        def func_key(func):
            name = func.get("name")
            file_path = os.path.abspath(func.get("file_path", func.get("file", "")))
            lineno = func.get("lineno", func.get("line_number", 0))
            return (name, file_path, lineno)
        # æ¤œç´¢çµæœã®ã‚­ãƒ¼é †ãƒªã‚¹ãƒˆ
        search_func_keys = [
            (r["name"], os.path.abspath(r.get("file", "")), r.get("lineno", r.get("line_number", 0)))
            for r in search_results
        ]
        # standalone_functions ã‚’æ¤œç´¢çµæœã®é †ã«ä¸¦ã¹ã‚‹
        func_key_to_func = {func_key(f): f for f in standalone_functions}
        sorted_funcs = []
        used_keys = set()
        for k in search_func_keys:
            if k in func_key_to_func and k not in used_keys:
                sorted_funcs.append(func_key_to_func[k])
                used_keys.add(k)
        # æ®‹ã‚Šï¼ˆæ¤œç´¢çµæœã«å‡ºã¦ã“ãªã„ã‚‚ã®ï¼‰
        for f in standalone_functions:
            k = func_key(f)
            if k not in used_keys:
                sorted_funcs.append(f)
        standalone_functions = sorted_funcs
        # --- ã“ã“ã¾ã§è¿½åŠ  ---
        
        return {
            "classes": sorted_classes,
            "standalone_functions": standalone_functions,
            "total_classes": len(classes),
            "total_standalone_functions": len(standalone_functions),
            "search_query": request.query,
            "search_results_count": len(search_results),
            "scoring_method": "composite_score",
            "scoring_description": "Classes (per file) ranked by composite score: weighted_score Ã— (1 + proportion_bonus). This combines ranking quality (âˆ‘(1/rank)) with hit proportion to favor classes with both high-ranking methods and good coverage."
        }
    except Exception as e:
        print(f"Error getting class stats: {e}")
        raise HTTPException(status_code=500, detail=str(e))



# ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªå˜ä½ã§ã‚¯ãƒ©ã‚¹ã‚¿åˆ†å‰²
# ä¾‹: src/ â†’ cluster_src, tests/ â†’ cluster_tests

def get_clusters_for_directory(directory: str, file_ext: str = ".py"):
    clusters = {}
    for root, dirs, files in os.walk(directory):
        rel_root = os.path.relpath(root, directory)
        if rel_root == ".":
            cluster_name = "root"
        else:
            cluster_name = rel_root.replace(os.sep, "_")
        cluster_files = [os.path.join(root, f) for f in files if f.endswith(file_ext)]
        if cluster_files:
            clusters[cluster_name] = cluster_files
    return clusters

# ã‚¯ãƒ©ã‚¹ã‚¿ã”ã¨ã« .owl_index/cluster_xxx/ ã‚’ä½œæˆ

def get_cluster_index_path(base_dir: str, cluster_name: str, file_ext: str = ".py") -> str:
    ext_dir = file_ext.lstrip(".")
    return os.path.join(base_dir, OWL_INDEX_DIR, f"cluster_{cluster_name}", ext_dir)

# ã‚¯ãƒ©ã‚¹ã‚¿ä¸€è¦§ã‚’ç®¡ç†
class ClusterManager:
    def __init__(self, base_dir: str, file_ext: str = ".py"):
        self.base_dir = base_dir
        self.file_ext = file_ext
        self.clusters = get_clusters_for_directory(base_dir, file_ext)
        self.cluster_indexes = {}
        for cname in self.clusters:
            cpath = get_cluster_index_path(base_dir, cname, file_ext)
            # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯ä½œæˆã›ãšã€å¿…è¦æ™‚ã«ä½œæˆ
            self.cluster_indexes[cname] = ClusterIndex(cname, Path(cpath))

    def get_cluster_for_file(self, file_path: str) -> str:
        rel = os.path.relpath(file_path, self.base_dir)
        parts = rel.split(os.sep)
        if len(parts) == 1:
            return "root"
        return parts[0]

    def get_all_clusters(self):
        return self.cluster_indexes.values()

    def get_changed_clusters(self) -> Dict[str, tuple[List[str], List[str]]]:
        """å„ã‚¯ãƒ©ã‚¹ã‚¿ã§å¤‰æ›´ã•ã‚ŒãŸãƒ•ã‚¡ã‚¤ãƒ«ã‚’å–å¾—"""
        changed_clusters = {}
        for cluster_name, files in self.clusters.items():
            cluster = self.cluster_indexes[cluster_name]
            added_or_modified, deleted = cluster.get_changed_files(files)
            if added_or_modified or deleted:
                changed_clusters[cluster_name] = (added_or_modified, deleted)
        return changed_clusters

    def update_cluster_incrementally(self, cluster_name: str, added_files: List[str], deleted_files: List[str]):
        """ã‚¯ãƒ©ã‚¹ã‚¿ã‚’å·®åˆ†æ›´æ–°"""
        cluster = self.cluster_indexes[cluster_name]
        
        # è¿½åŠ ãƒ»å¤‰æ›´ãƒ•ã‚¡ã‚¤ãƒ«ã‹ã‚‰é–¢æ•°ã‚’æŠ½å‡º
        added_funcs = []
        for file_path in added_files:
            try:
                funcs = extract_functions(file_path)
                for func in funcs:
                    func['file'] = file_path
                added_funcs.extend(funcs)
            except Exception as e:
                print(f"[ClusterManager] extract error: {file_path}: {e}")
        
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆï¼ˆè¿½åŠ åˆ†ã®ã¿ï¼‰
        embeddings = None
        if added_funcs:
            codes = [func["code"] for func in added_funcs]
            if codes:
                embeddings = encode_with_memory_management(codes, settings.batch_size)
        
        # ã‚¯ãƒ©ã‚¹ã‚¿ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹ã‚’æ›´æ–°
        cluster.update_files(added_funcs, deleted_files, embeddings)
        print(f"[ClusterManager] Updated cluster '{cluster_name}': +{len(added_files)} files, -{len(deleted_files)} files")

    def rebuild_cluster(self, cluster_name: str):
        """ã‚¯ãƒ©ã‚¹ã‚¿ã‚’å®Œå…¨å†æ§‹ç¯‰"""
        cluster = self.cluster_indexes[cluster_name]
        files = self.clusters.get(cluster_name, [])
        
        # é–¢æ•°æŠ½å‡º
        funcs = []
        for f in files:
            try:
                funcs_in_file = extract_functions(f)
                for func in funcs_in_file:
                    func['file'] = f
                funcs.extend(funcs_in_file)
            except Exception as e:
                print(f"[ClusterManager] extract error: {f}: {e}")
        
        # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
        embeddings = None
        if funcs:
            codes = [func["code"] for func in funcs]
            if codes:
                get_device_and_prepare()
                encode = partial(model.encode, show_progress_bar=True)
                embeddings = encode(codes, batch_size=settings.batch_size, convert_to_numpy=True)
        
        # ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰
        cluster.rebuild_from_functions(funcs, embeddings)
        print(f"[ClusterManager] Rebuilt cluster '{cluster_name}': {len(funcs)} functions from {len(files)} files")

    def update_all_clusters(self, force_rebuild: bool = False):
        """å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã‚’æ›´æ–°ï¼ˆå·®åˆ†ã¾ãŸã¯å®Œå…¨å†æ§‹ç¯‰ï¼‰"""
        if force_rebuild:
            for cluster_name in self.clusters:
                self.rebuild_cluster(cluster_name)
        else:
            changed_clusters = self.get_changed_clusters()
            for cluster_name, (added_files, deleted_files) in changed_clusters.items():
                if deleted_files:
                    # å‰Šé™¤ãŒã‚ã£ãŸå ´åˆã¯å®Œå…¨å†æ§‹ç¯‰
                    print(f"[ClusterManager] Files deleted in cluster '{cluster_name}', rebuilding...")
                    self.rebuild_cluster(cluster_name)
                else:
                    # è¿½åŠ ã®ã¿ã®å ´åˆã¯å·®åˆ†æ›´æ–°
                    self.update_cluster_incrementally(cluster_name, added_files, deleted_files)

    def search_in_clusters(self, query: str, top_k: int = 5) -> List[Dict]:
        """å…¨ã‚¯ãƒ©ã‚¹ã‚¿ã§æ¤œç´¢ã‚’å®Ÿè¡Œ"""
        get_device_and_prepare()
        query_emb = model.encode([query], batch_size=settings.batch_size, convert_to_numpy=True)
        
        all_results = []
        for cluster in self.cluster_indexes.values():
            results = cluster.search(query_emb, top_k)
            all_results.extend(results)
        
        # ã‚¹ã‚³ã‚¢é †ã§ã‚½ãƒ¼ãƒˆï¼ˆã“ã“ã§ã¯ç°¡å˜ãªå®Ÿè£…ï¼‰
        return all_results[:top_k]

    def rebuild_changed_clusters(self):
        # å¤‰æ›´ãŒã‚ã£ãŸãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªï¼ˆã‚¯ãƒ©ã‚¹ã‚¿ï¼‰ã¯å•ç­”ç„¡ç”¨ã§å†æ§‹ç¯‰
        print(f"ğŸ”„ Rebuilding changed clusters...")
        for cname, files in tqdm(self.clusters.items(), desc="Rebuilding clusters"):
            cluster = self.cluster_indexes[cname]
            # é–¢æ•°æŠ½å‡º
            funcs = []
            for f in files:
                try:
                    funcs.extend(extract_functions(f))
                except Exception as e:
                    print(f"[ClusterManager] extract error: {f}: {e}")
            # åŸ‹ã‚è¾¼ã¿ç”Ÿæˆ
            codes = [func["code"] for func in funcs]
            if codes:
                print(f"ğŸ”„ Generating embeddings for cluster '{cname}' ({len(codes)} functions)...")
                embeddings = encode_with_memory_management(codes, settings.batch_size, show_progress=True)
                # FAISSã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å†æ§‹ç¯‰
                faiss_index = faiss.IndexFlatL2(embeddings.shape[1])
                faiss_index.add(embeddings)
                cluster.meta = funcs
                cluster.index = faiss_index
                cluster.save()
            else:
                # ãƒ•ã‚¡ã‚¤ãƒ«ãŒç©ºãªã‚‰ã‚¤ãƒ³ãƒ‡ãƒƒã‚¯ã‚¹å‰Šé™¤
                cluster.meta = []
                cluster.index = None
                cluster.save()

    def rebuild_clusters_with_diff(self, changed_dirs: set, added_files: set):
        # è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã¯å·®åˆ†è¿½åŠ ã€å¤‰æ›´ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã¯å•ç­”ç„¡ç”¨ã§å†æ§‹ç¯‰
        print(f"ğŸ”„ Rebuilding clusters with differential updates...")
        for cname, files in tqdm(self.clusters.items(), desc="Processing clusters"):
            cluster = self.cluster_indexes[cname]
            cluster_dir = os.path.join(self.base_dir, *(cname.split('_'))) if cname != 'root' else self.base_dir
            if cluster_dir in changed_dirs:
                # ãƒ‡ã‚£ãƒ¬ã‚¯ãƒˆãƒªã”ã¨å†æ§‹ç¯‰
                funcs = []
                for f in files:
                    try:
                        funcs.extend(extract_functions(f))
                    except Exception as e:
                        print(f"[ClusterManager] extract error: {f}: {e}")
                codes = [func["code"] for func in funcs]
                if codes:
                    print(f"ğŸ”„ Full rebuild for cluster '{cname}' ({len(codes)} functions)...")
                    get_device_and_prepare()
                    encode = partial(model.encode, show_progress_bar=True)
                    embeddings = encode(codes, batch_size=settings.batch_size, convert_to_numpy=True)
                    faiss_index = faiss.IndexFlatL2(embeddings.shape[1])
                    faiss_index.add(embeddings)
                    cluster.meta = funcs
                    cluster.index = faiss_index
                    cluster.save()
                else:
                    cluster.meta = []
                    cluster.index = None
                    cluster.save()
            else:
                # è¿½åŠ ãƒ•ã‚¡ã‚¤ãƒ«ã®ã¿å·®åˆ†è¿½åŠ 
                add_funcs = []
                add_files = [f for f in files if f in added_files]
                for f in add_files:
                    try:
                        add_funcs.extend(extract_functions(f))
                    except Exception as e:
                        print(f"[ClusterManager] extract error: {f}: {e}")
                if add_funcs:
                    codes = [func["code"] for func in add_funcs]
                    print(f"ğŸ”„ Adding {len(codes)} new functions to cluster '{cname}'...")
                    get_device_and_prepare()
                    encode = partial(model.encode, show_progress_bar=True)
                    embeddings = encode(codes, batch_size=settings.batch_size, convert_to_numpy=True)
                    if cluster.index is None and len(embeddings) > 0:
                        faiss_index = faiss.IndexFlatL2(embeddings.shape[1])
                        faiss_index.add(embeddings)
                        cluster.index = faiss_index
                        cluster.meta = add_funcs
                    else:
                        cluster.index.add(embeddings)
                        cluster.meta.extend(add_funcs)
                    cluster.save()

@app.get("/settings")
async def get_settings():
    """ç¾åœ¨ã®è¨­å®šå€¤ã‚’è¿”ã™API"""
    return {
        "batch_size": settings.batch_size,
        "device": get_device(),
        "model_device": model_device
    }

class UpdateSettingsRequest(BaseModel):
    batch_size: Optional[int] = None

@app.post("/update_settings")
async def update_settings(req: UpdateSettingsRequest):
    """è¨­å®šå€¤ã‚’å‹•çš„ã«æ›´æ–°ã™ã‚‹API"""
    if req.batch_size is not None:
        settings.batch_size = req.batch_size
    return {
        "message": "Settings updated",
        "batch_size": settings.batch_size
    }

@app.post("/set_batch_size")
async def set_batch_size(batch_size: int):
    """
    åŸ‹ã‚è¾¼ã¿ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’å‹•çš„ã«å¤‰æ›´ã™ã‚‹API
    """
    settings.batch_size = batch_size
    return {"batch_size": settings.batch_size}
