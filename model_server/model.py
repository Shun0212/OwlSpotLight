from sentence_transformers import SentenceTransformer
import numpy as np
import os
import torch
import gc
import warnings

# MPSãƒ‡ãƒã‚¤ã‚¹ä½¿ç”¨æ™‚ã®torch.compileè­¦å‘Šã‚’æŠ‘åˆ¶
warnings.filterwarnings("ignore", message=".*torch.compile.*mps.*", category=UserWarning)

DEFAULT_MODEL = "Shuu12121/CodeSearch-ModernBERT-Owl-3.0-Plus"
model_name = os.environ.get("OWL_MODEL_NAME", DEFAULT_MODEL)
# ç’°å¢ƒå¤‰æ•°ã§é€²æ—è¡¨ç¤ºã‚’åˆ¶å¾¡ ("0"/"false" ã§éè¡¨ç¤º)
progress_env = os.environ.get("OWL_PROGRESS", "1").lower()

# ã‚°ãƒ­ãƒ¼ãƒãƒ«å¤‰æ•°ã§ãƒ¢ãƒ‡ãƒ«ã¨ãƒ‡ãƒã‚¤ã‚¹ã‚’ç®¡ç†
model = None
model_device = None

def get_device():
    """åˆ©ç”¨å¯èƒ½ãªæœ€é©ãªãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—"""
    # Apple Silicon (M1/M2/M3) ãªã©ã§ mps ãŒä½¿ãˆã‚‹å ´åˆã¯ mps ã‚’å„ªå…ˆ
    if torch.backends.mps.is_available() and torch.backends.mps.is_built():
        return "mps"
    elif torch.cuda.is_available():
        return "cuda"
    else:
        return "cpu"

def cleanup_memory():
    """ãƒ¡ãƒ¢ãƒªã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚’å®Ÿè¡Œ"""
    if torch.backends.mps.is_available():
        torch.mps.empty_cache()
    elif torch.cuda.is_available():
        torch.cuda.empty_cache()
    gc.collect()

def load_model_with_device_fallback():
    """ãƒ¢ãƒ‡ãƒ«ã‚’é©åˆ‡ãªãƒ‡ãƒã‚¤ã‚¹ã§èª­ã¿è¾¼ã¿ã€å¿…è¦ã«å¿œã˜ã¦ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯"""
    global model, model_device
    
    if model is not None:
        return model
    
    print(f"ğŸ¦‰ Loading model: {model_name}")
    
    try:
        # ã¾ãšã¯ãƒ¢ãƒ‡ãƒ«ã‚’èª­ã¿è¾¼ã¿
        model = SentenceTransformer(model_name)
        
        # ãƒ‡ãƒã‚¤ã‚¹ã‚’æ±ºå®šã—ã¦ç§»å‹•
        device = get_device()
        print(f"ğŸ”§ Attempting to use device: {device}")
        
        # MPSãƒ‡ãƒã‚¤ã‚¹ã®å ´åˆã€torch.compileã‚’ç„¡åŠ¹åŒ–ã—ã¦warningã‚’é˜²ã
        if device == "mps":
            # SentenceTransformerã®å†…éƒ¨ã§torch.compileãŒä½¿ç”¨ã•ã‚Œãªã„ã‚ˆã†è¨­å®š
            if hasattr(model, '_modules'):
                for module in model._modules.values():
                    if hasattr(module, '_is_compiled'):
                        module._is_compiled = False
        
        model.to(device)
        model_device = device
        print(f"âœ… Model loaded successfully on {device}")
        emb_dim = model.get_sentence_embedding_dimension()
        print(f"â„¹ï¸ Embedding dimension: {emb_dim}")
        
    except Exception as e:
        print(f"âš ï¸ Failed to load model on {device}: {e}")
        
        # MPSã§å¤±æ•—ã—ãŸå ´åˆã¯CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
        if device == "mps":
            print("ğŸ”„ Falling back to CPU...")
            try:
                if model is not None:
                    model.to("cpu")
                else:
                    model = SentenceTransformer(model_name)
                    model.to("cpu")
                model_device = "cpu"
                cleanup_memory()
                print("âœ… Model loaded successfully on CPU")
            except Exception as cpu_e:
                print(f"âŒ Failed to load model on CPU: {cpu_e}")
                raise cpu_e
        else:
            raise e
    
    return model

def get_model():
    """ãƒ¢ãƒ‡ãƒ«ã‚¤ãƒ³ã‚¹ã‚¿ãƒ³ã‚¹ã‚’å–å¾—ï¼ˆé…å»¶èª­ã¿è¾¼ã¿ï¼‰"""
    if model is None:
        load_model_with_device_fallback()
    return model

def encode_code(codes: list[str], batch_size: int = 8, max_retries: int = 3, show_progress: bool = True) -> np.ndarray:
    """ã‚³ãƒ¼ãƒ‰ã‚’ã‚¨ãƒ³ã‚³ãƒ¼ãƒ‰ã—ã€ãƒ¡ãƒ¢ãƒªã‚¨ãƒ©ãƒ¼æ™‚ã¯è‡ªå‹•çš„ã«ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’èª¿æ•´"""
    from tqdm import tqdm
    import sys

    current_model = get_model()

    # ç’°å¢ƒå¤‰æ•°ã‚„ä»¶æ•°ã«å¿œã˜ã¦é€²æ—è¡¨ç¤ºã‚’åˆ¶å¾¡
    progress_enabled = (
        show_progress
        and progress_env not in ("0", "false")
        and len(codes) >= 10
    )

    for attempt in range(max_retries):
        try:
            return current_model.encode(
                codes,
                batch_size=batch_size,
                normalize_embeddings=True,
                show_progress_bar=progress_enabled,
                convert_to_numpy=True
            )
        
        except (RuntimeError, torch.cuda.OutOfMemoryError) as e:
            error_msg = str(e).lower()
            if "memory" in error_msg or "out of memory" in error_msg:
                print(f"âš ï¸ Memory error on attempt {attempt + 1}: {e}")
                cleanup_memory()
                
                if attempt < max_retries - 1:
                    # ãƒãƒƒãƒã‚µã‚¤ã‚ºã‚’åŠåˆ†ã«æ¸›ã‚‰ã—ã¦å†è©¦è¡Œ
                    batch_size = max(1, batch_size // 2)
                    print(f"ğŸ”„ Reducing batch size to {batch_size} and retrying...")
                    
                    # æœ€å¾Œã®è©¦è¡Œã§MPSãŒå¤±æ•—ã—ãŸå ´åˆã€CPUã«ãƒ•ã‚©ãƒ¼ãƒ«ãƒãƒƒã‚¯
                    if attempt == max_retries - 2 and model_device == "mps":
                        print("ğŸ”„ Falling back to CPU due to persistent MPS memory issues...")
                        current_model.to("cpu")
                        model_device = "cpu"
                        cleanup_memory()
                else:
                    print("âŒ All memory optimization attempts failed")
                    raise e
            else:
                raise e
    
    raise RuntimeError("Failed to encode after all retries")

def get_model_embedding_dim() -> int:
    """ãƒ¢ãƒ‡ãƒ«ã®åŸ‹ã‚è¾¼ã¿æ¬¡å…ƒæ•°ã‚’å–å¾—"""
    current_model = get_model()
    return current_model.get_sentence_embedding_dimension()

def get_current_device() -> str:
    """ç¾åœ¨ä½¿ç”¨ä¸­ã®ãƒ‡ãƒã‚¤ã‚¹ã‚’å–å¾—"""
    return model_device if model_device else "not_loaded"
